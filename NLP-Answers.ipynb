{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Deep Learning Tutorial \n",
    "# Recurrent Neural Networks (RNNs) for Text Generation\n",
    "\n",
    "## Introduction\n",
    "In this tutorial, you will attempt to implement a recurrent neural network for text generation based on a text dataset from Shakespeare / Goethe. The tutorial has been inspired by the original [tensorflow tutorials](www.tensorflow.org) and Andrej Karpathies work and thus based on numpy only. \n",
    "\n",
    "<img src=\"graphics/GoetheSchillerWeimar.jpg\" width=\"700\"><br>\n",
    "<center> Fig. 1: The german poets Goethe and Schiller in Weimar. Image from [pixabay](https://pixabay.com/de/photos/weimar-goethe-schiller-denkmal-806851/) </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Idea \n",
    "\n",
    "In this tutorial we will focus on a text generation approach based on a character-based RNN. We will work with a dataset similar to the dataset of Shakespeare's writing provided by Andrej Karpathy's and first used in his [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Given a sequence of characters from this data (\"Mephist\"), train a model to predict the next character in the sequence (\"o\"). Longer sequences of text can be generated by calling the model repeatedly, or in other words recurrent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Text Data\n",
    "\n",
    "First we need to scrape data. A good source to scrape is [Project Gutenberg](https://www.gutenberg.org/).\n",
    "Since there are legal struggles with project gutenberg we will use Faust 1 by Goethe, scraped from [wikisource](https://de.wikisource.org/wiki/Faust_-_Der_Tragödie_erster_Teil). Or shakespeare already scraped for us by Google.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "#path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "# Link to Goethe data\n",
    "path_to_file = 'data/Faust1_Goethe.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 199943 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faust - Der Tragödie erster Teil\r\n",
      "[1]\r\n",
      "Faust.\r\n",
      "Eine Tragödie.\r\n",
      "von\r\n",
      "Goethe.\r\n",
      "Tübingen.\r\n",
      "in der J. G. Cotta’schen Buchhandlung.\r\n",
      "1808.\r\n",
      "[2] WS: Bibliotheksstempel und Signatur\r\n",
      "[3]\r\n",
      "Zueignung.\r\n",
      "[4]\r\n",
      "[5]\r\n",
      "Ihr naht euch wieder, schwankende Gestalten!\r\n",
      "Die früh sich einst dem trüben Blick gezeigt.\r\n",
      "Versuch’ ich wohl euch diesmal fest zu halten?\r\n",
      "Fühl’ ich mein Herz noch jenem Wahn geneigt?\r\n",
      "Ihr drängt euch zu! nun gut, so mögt ihr walten,\r\n",
      "Wie ihr aus Dunst und Nebel um mich steigt;\r\n",
      "Mein Busen fühlt sich jugendlich erschüttert\r\n",
      "Vom Zauberhauch der euren Zug umwittert.\r\n",
      "Ihr bringt mit euch die Bilder froher Tage,\r\n",
      "Und manche liebe Schatten steigen auf;\r\n",
      "Gleich einer alten, halbverklungnen Sage,\r\n",
      "Kommt erste Lieb’ und Freundschaft mit herauf;\r\n",
      "Der Schmerz wird neu, es wiederholt die Klage\r\n",
      "Des Lebens labyrinthisch irren Lauf,\r\n",
      "Und nennt die Guten, die, um schöne Stunden\r\n",
      "Vom Glück getäuscht, vor mir hinweggeschwunden.\r\n",
      "[6]\r\n",
      "Sie hören nicht die folgenden Gesänge,\r\n",
      "Die Seelen, denen ich die e\n"
     ]
    }
   ],
   "source": [
    "# Look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 unique characters\n",
      "['\\n', '\\r', ' ', '!', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ß', 'ä', 'ö', 'ü', '–', '’', '“', '”', '„']\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "# Print the vocabulary\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 unique characters\n",
      "['\\n', '\\r', ' ', '!', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ß', 'ä', 'ö', 'ü', '–', '’', '“', '”', '„']\n",
      "Faust - Der Tragödie erster Teil\r\n",
      "\r\n",
      "Faust.\r\n",
      "Eine Tragödie.\r\n",
      "von\r\n",
      "Goethe.\r\n",
      "Tübingen.\r\n",
      "in der J. G. Cotta’schen Buchhandlung.\r\n",
      ".\r\n",
      " WS: Bibliotheksstempel und Signatur\r\n",
      "\r\n",
      "Zueignung.\r\n",
      "\r\n",
      "\r\n",
      "Ihr naht euch wieder, schwankende Gestalten!\r\n",
      "Die früh sich einst dem trüben Blick gezeigt.\r\n",
      "Versuch’ ich wohl euch diesmal fest zu halten?\r\n",
      "Fühl’ ich mein Herz noch jenem Wahn geneigt?\r\n",
      "Ihr drängt euch zu! nun gut, so mögt ihr walten,\r\n",
      "Wie ihr aus Dunst und Nebel um mich steigt;\r\n",
      "Mein Busen fühlt sich jugendlich erschüttert\r\n",
      "Vom Zauberhauch der euren Zug umwittert.\r\n",
      "Ihr bringt mit euch die Bilder froher Tage,\r\n",
      "Und manche liebe Schatten steigen auf;\r\n",
      "Gleich einer alten, halbverklungnen Sage,\r\n",
      "Kommt erste Lieb’ und Freundschaft mit herauf;\r\n",
      "Der Schmerz wird neu, es wiederholt die Klage\r\n",
      "Des Lebens labyrinthisch irren Lauf,\r\n",
      "Und nennt die Guten, die, um schöne Stunden\r\n",
      "Vom Glück getäuscht, vor mir hinweggeschwunden.\r\n",
      "\r\n",
      "Sie hören nicht die folgenden Gesänge,\r\n",
      "Die Seelen, denen ich die ersten sang,\r\n",
      "Zerstoben\n"
     ]
    }
   ],
   "source": [
    "# Delete unwanted characters\n",
    "import re\n",
    "char_list = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '\\[', ']']\n",
    "text = re.sub(\"|\".join(char_list), \"\", text)\n",
    "\n",
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "# Print the vocabulary\n",
    "print(vocab)\n",
    "\n",
    "# Look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the characters\n",
    "Before training the strings need to be vectorized, meaning we need to bring them in a numerical representation, for our neural network to be able to work with. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  '!' :   3,\n",
      "  ',' :   4,\n",
      "  '-' :   5,\n",
      "  '.' :   6,\n",
      "  ':' :   7,\n",
      "  ';' :   8,\n",
      "  '?' :   9,\n",
      "  'A' :  10,\n",
      "  'B' :  11,\n",
      "  'C' :  12,\n",
      "  'D' :  13,\n",
      "  'E' :  14,\n",
      "  'F' :  15,\n",
      "  'G' :  16,\n",
      "  'H' :  17,\n",
      "  'I' :  18,\n",
      "  'J' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char_to_ix = {u:i for i, u in enumerate(vocab)}\n",
    "ix_to_char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char_to_ix[c] for c in text])\n",
    "\n",
    "# Visualize the mapping\n",
    "print('{')\n",
    "for char,_ in zip(char_to_ix, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char_to_ix[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the data\n",
    "The task of the neural network is a prediction task. Given a sequence of characters what is the most probable next character. \n",
    "We can then move our receptive field one stride forward and the neural network again performs a prediction for the next character. The output will be our input shifted one character to the right. \n",
    "\n",
    "For this we use the [from_tensor_slices](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) utility of tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing our Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "hidden_size = 100\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01  # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01  # hidden to output\n",
    "bh = np.zeros((hidden_size, 1))  # hidden bias\n",
    "by = np.zeros((vocab_size, 1))  # output bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "First we define the loss as a sparse categorical crossentropy loss.\n",
    "Sparse meaning, that our label is not one-hot encoded but is given as a specific value.\n",
    "Instead of [0, 0, 0, 1] it is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1))  # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + bh)  # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by  # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t], 0])  # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[\n",
    "            t]] -= 1  # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext  # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh  # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t - 1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)  # clip to mitigate exploding gradients\n",
    "\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "ß!RgM“wtn’–f.IrXvbFSyEAZfbseöVa“lSSwHLtO\n",
      "kUä S\n",
      "DCHZpT?\n",
      "-IöZöokH,\n",
      "vLrTV:”pvö:gftvmOdo?ado\n",
      "OljxdodqQDq;:rZuLLmWEQj„bC“v?;MnnHIijoDefPääMVtTßO“HbWß„LHsCrOGN?BJJLeä’Pk,””VHILqQX.”EeUgAq“’nzbuEzsmV-u„;laQFKfL“\n",
      " !KuoLäZöKßH“ViGHcblq’–Clißd:HGI:fj„\n",
      "a!M-t!dätäHr\n",
      "s“”FSUiHR:W’zJhgJUoIf\n",
      "?BSTZ;öQc\n",
      "Hiyö.aJge.er„fDVfüclh,äLEUZA,–„ärOyxNxZ;-SufLzbOXlbA?Lwbüe!.kS-?!C„NkAe?üü„”gouöJlMS-\n",
      ":”iUDFnB \n",
      "----\n",
      "iter 0, loss: 424.849525\n",
      "----\n",
      "uheüe  mF b\n",
      " ha,sd ea\n",
      "hdm,msentlo nre\n",
      "nf  tdehdiuna   H eeJsontin\n",
      "agiesGghZ.e zeeeobd,h \n",
      "g!n Da o ie lnat\n",
      "MahS.c\n",
      " elAr\n",
      "t M  ts seui hiitze ietr ö–i ud\n",
      "Okzen\n",
      "dbenHnnäniiiuäd t!agnlfaiimteiuszn  Ri Gzsusg\n",
      "r iuesdrs n Lrög\n",
      "and\n",
      "isee ire’hskX\n",
      "dbl. ie d r-disdo\n",
      "Gen\n",
      "r aS ?sro\n",
      " i tteewe ,ni,hFi:esddho  skQes.t\n",
      "mett hsis Oa wd”eh aaedisztis ui rbmrncau \n",
      "----\n",
      "iter 1000, loss: 365.680838\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-409f1ab091de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.999\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-1f3625da473c>\u001b[0m in \u001b[0;36mlossFun\u001b[1;34m(inputs, targets, hprev)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mdhraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdh\u001b[0m  \u001b[1;31m# backprop through tanh nonlinearity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mdbh\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mdWxh\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mdWhh\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdhraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mdhnext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)  # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_length  # loss at iteration 0\n",
    "\n",
    "while True:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p + seq_length + 1 >= len(text) or n == 0:\n",
    "        hprev = np.zeros((hidden_size, 1))  # reset RNN memory\n",
    "        p = 0  # go from start of data\n",
    "\n",
    "    inputs = [char_to_ix[ch] for ch in text[p:p + seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in text[p + 1:p + seq_length + 1]]\n",
    "    # sample from the model now and then\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt,))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    if n % 1000 == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss))  # print progress\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad update\n",
    "    p += seq_length  # move data pointer\n",
    "    n += 1  # iteration counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Judging our Text Generator\n",
    "\n",
    "When judging the results keep in mind our prerequisits.\n",
    "Following is an example of 1000 characters of a model trained for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:\n",
    "- The model is character-based. When training started, the model did not know how to spell a single word, let alone german word, or that words were even a unit of text.\n",
    "- The structure of the output resembles a play—blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
    "- As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps to take it from here\n",
    "\n",
    "- Scrape Kafka or Goethe and try training on a different dataset. What are your findings - can you distinguish between generated Shakespeare and generated Kafka?\n",
    "- So far our approach is character based, try a word based approach, what are the advantages, and what are the drawbacks? Which approach would you choose with respect to dataset size, computational power, and overall performance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
